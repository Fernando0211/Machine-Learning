{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron intuition**"
      ],
      "metadata": {
        "id": "-ZzO7N23PQYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A perceptron emulates the function of a neuron in the\n",
        "brain. It forms the fundamental building block of a multilayer\n",
        "perceptron, commonly known as a neural network. In its\n",
        "operation, the perceptron multiplies input values by weights\n",
        "and then adds a bias. This resulting value is then processed\n",
        "through an activation function, which decides the output clas-\n",
        "sification. Notably, a single perceptron is designed for binary\n",
        "classification, distinguishing between two possible outcomes.\n",
        "\n",
        "* Weighted Inputs: A perceptron multiplies each input by a weight.\n",
        "\n",
        "* Bias: This is a constant shift, letting the perceptron make decisions not just using input values, but giving the perceptron an innate tendency.\n",
        "\n",
        "* Activation Function: This converts the sum of weighted inputs and bias into a binary decision (e.g., 1 or 0)\n",
        "\n",
        "*Optimization function: This enhance the weights and bias in relation with the error bewteen the prediction and the actual label\n"
      ],
      "metadata": {
        "id": "DoaWWVQoPUy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pseudocode**"
      ],
      "metadata": {
        "id": "5VwOXYlVRl8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "1.Read dataset.\n",
        "2. Dataset Preprocessing:\n",
        "    -Drop unnecessary columns if needed.\n",
        "    -Replace text with numeric values.\n",
        "    -Generate dummies if needed.\n",
        "    -Normalize or scale features.\n",
        "3.Dataset split (80% for training and 20% for test).\n",
        "4.Get the features (X) and labels (Y).\n",
        "\n",
        "5. Functions determination\n",
        "  Accuracy(predictions, true_values):\n",
        "      for len(true_values):\n",
        "        if predictions == true_values:\n",
        "          count += 1\n",
        "      return accuracy = count/len(true_values)\n",
        "\n",
        "  StepFunction(input_value):\n",
        "    if input_value >= 0:\n",
        "        return 1\n",
        "    else\n",
        "        return 0\n",
        "\n",
        "  Optimization(error, data, weights, bias, learning_rate):\n",
        "    Weights = Formula*\n",
        "    Bias = Fomula*\n",
        "    return Weights, Bias\n",
        "\n",
        "  Training(learning_rate, epochs, training_data, training_labels):\n",
        "    Initialize weights and bias\n",
        "    for epochs:\n",
        "        for training_data:\n",
        "            y = weights * training_data + bias\n",
        "            prediction = StepFunction(y)\n",
        "            if error:\n",
        "                Optimization()\n",
        "        \n",
        "    return final weights and bias\n",
        "\n",
        "    Test(final weights, final bias, test_data, test_labels):\n",
        "      for len(x_test):\n",
        "          y =  test_data * final_weights + final_bias\n",
        "          prediction = StepFunction(y)\n",
        "      accuracy = Accuracy()\n",
        "\n",
        "    return prediction, accuracy\n",
        "\n",
        "6. Train\n",
        "training(testing_data, testing_labels, weights, bias)\n",
        "\n",
        "7. Test\n",
        "\n",
        "Test(final weights, final bias, test_data, test_labels)\n",
        "Get accuaracy\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sLpQKhMjRqnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron Implementation**"
      ],
      "metadata": {
        "id": "ud6_917YVIgc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M3jwTbMEfoJ"
      },
      "outputs": [],
      "source": [
        "#Libraries to read and modify the dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'winequality-red.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Change regresion problem to classification problem\n",
        "def convert_quality(value):\n",
        "    if 1 <= value <= 5:\n",
        "        return 0\n",
        "    elif 6 <= value <= 10:\n",
        "        return 1\n",
        "\n",
        "df['quality'] = df['quality'].apply(convert_quality)\n",
        "\n",
        "X = df.drop('quality', axis=1) # Get features\n",
        "Y = df['quality'] # Get labels\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Split data in train and test sets\n",
        "\n",
        "for column in X_train.columns:    # Normalize datasets\n",
        "    min_val = X_train[column].min()\n",
        "    max_val = X_train[column].max()\n",
        "\n",
        "    X_train[column] = (X_train[column] - min_val) / (max_val - min_val)\n",
        "\n",
        "for column in X_test.columns:\n",
        "    min_val = X_test[column].min()\n",
        "    max_val = X_test[column].max()\n",
        "\n",
        "    X_test[column] = (X_test[column] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Get arrays from the pandas dataframes\n",
        "\"\"\"\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "Y_train = Y_train.values\n",
        "Y_test = Y_test.values\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hZD-Jx9XN3Gg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4f75d668-ee77-4a11-f779-e0f839351e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX_train = X_train.values\\nX_test = X_test.values\\nY_train = Y_train.values\\nY_test = Y_test.values\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(prediction, Y_test):\n",
        "  correct = 0 #Define a variable to store the count of correct predictions\n",
        "  for i in range(len(prediction)):\n",
        "    if prediction[i] == Y_test[i]:  #If is the same label it will add 1 to the variable\n",
        "      correct += 1\n",
        "\n",
        "  accuracy = (correct/len(Y_test))*100 #Divide the correct predictions by the numbers of labels\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def step(y):  # Activation function. Values lesser than 0 become 0, it becomes 1 for values equal to 0 or greater\n",
        "  if y >= 0:\n",
        "    f = 1\n",
        "  else:\n",
        "    f = 0\n",
        "  return f\n",
        "\n",
        "\n",
        "def opt(error, x_train, w, b, learning_rate): # Optimization functions. It depends on the error sign and the variation by the learning rate\n",
        "\n",
        "  Nweights = w + learning_rate * error * x_train\n",
        "  Nbias = b + learning_rate * error\n",
        "\n",
        "  return Nweights, Nbias\n",
        "\n",
        "\n",
        "def training(learning_rate, epochs, x_train, y_train):\n",
        "    num_samples, num_features = X_train.shape  # Get dataset dimensions\n",
        "    weights = np.zeros(num_features)  # Initialize weights to zero\n",
        "    bias = 0  # Start with a bias of zero\n",
        "\n",
        "    for i in range(epochs):\n",
        "        predictions = []  # Store predictions for this epoch\n",
        "        error_t = 0  # Track error for this epoch\n",
        "\n",
        "        for j in range(len(x_train)):\n",
        "            y = sum(x_train[j] * weights) + bias  # Calculate raw output\n",
        "            predict = step(y)  # Apply activation function\n",
        "            predictions.append(predict)\n",
        "\n",
        "            error = y_train[j] - predict  # Compute prediction error\n",
        "            error_t += abs(error)  # Track total error for this epoch\n",
        "\n",
        "            if error != 0:  # Adjust weights and bias if there's an error\n",
        "                weights, bias = opt(error, x_train[j], weights, bias, learning_rate)\n",
        "\n",
        "        Merror = error_t/len(x_train)  # Calculate mean error for this epoch\n",
        "        print(\"Epoch: \", i, \"Mean Error: \", Merror)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "\n",
        "def test(x_test, y_test, final_weights, final_bias):\n",
        "    predictions_test = []  # Store predictions for test set\n",
        "\n",
        "    for j in range(len(x_test)):\n",
        "        y = sum(x_test[j] * final_weights) + final_bias  # Calculate raw output\n",
        "        predict = step(y)  # Apply activation function\n",
        "        predictions_test.append(predict)\n",
        "\n",
        "    final_accuracy = accuracy(predictions_test, y_test)  # Compute test accuracy\n",
        "\n",
        "    return predictions_test, final_accuracy"
      ],
      "metadata": {
        "id": "JAoZjXyw_N2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fw, fb = training(0.1, 500, X_train, Y_train)"
      ],
      "metadata": {
        "id": "_MaIn1zvZe7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_test, accurate = test(X_test, Y_test, fw, fb)\n",
        "accurate"
      ],
      "metadata": {
        "id": "sNNAkCpntHeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21513ba9-d43f-4e0d-df54-817351f886f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73.4375"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss and Optimization functions**"
      ],
      "metadata": {
        "id": "hgaD32qXVRyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Activation Function, is a Step Function. It operates in a simple way, if the input is greater than or equal to 0, the function returns 1; otherwise, it returns 0. This behavior facilitates binary classification, enabling the perceptron to distinctly categorize data into one of two classes.\n",
        "\n",
        "The Optimization Function, employs the perceptron learning rule.\n",
        "\n",
        "Weights Update:\n",
        "\n",
        "w_new = w_old + lr * error * input\n",
        "\n",
        "Bias Update:\n",
        "\n",
        "b_new = b_old + lr * error\n",
        "\n",
        "The adjustments to the model parameters are influenced by the error, learning rate, and the input data. There is an implicit loss function, represented by the error, that is the difference of the prediction and the actual label.\n",
        "\n",
        "error = y_test - predicted_label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lHYt6Qu6VWNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynxg_w2rX5Na",
        "outputId": "1aac1a55-60fa-4e70-f4e6-7ee5f87b0cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjzASiUiX66E",
        "outputId": "8a99da7d-34de-402f-fd70-588ab8e81eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html ///content/Perceptron.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSwswybyX9ai",
        "outputId": "75ce506a-b397-406e-bb96-618f8c02e346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook ///content/Perceptron.ipynb to html\n",
            "[NbConvertApp] Writing 621768 bytes to /content/Perceptron.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}